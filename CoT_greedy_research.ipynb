{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1J_2EneGCALTG0vG-I-Tzwif_HnBq5V7k",
      "authorship_tag": "ABX9TyMKWECu/QeU791jeU6kxugU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1croZavr/CoTResearch/blob/master/CoT_greedy_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка и импортирование необходимых библиотек и git clone репозитория с необходимым кодом и данными"
      ],
      "metadata": {
        "id": "THdRTvEhacxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PnfvS_aPQWX"
      },
      "outputs": [],
      "source": [
        "# %pip install -q petals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/M1croZavr/CoTResearch.git"
      ],
      "metadata": {
        "id": "OYIT9JN2ZfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "IjH5Z93JjAsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты сохранял на гугл диск, можно не выполнять ячейку, если не надо сохранять результаты на диск"
      ],
      "metadata": {
        "id": "3ceUozdJ8nfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xBqpjCxjqnFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import json\n",
        "import shutil\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "# from transformers import BloomTokenizerFast, set_seed\n",
        "# from petals import DistributedBloomForCausalLM\n",
        "from CoTResearch.data_preprocessing import FormattedPrompts, FormattedInputs\n",
        "from CoTResearch.data_postprocessing import AnswersList"
      ],
      "metadata": {
        "id": "VMx-JO37PTJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running device: {DEVICE}')"
      ],
      "metadata": {
        "id": "c8KkQZwvZQLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка модели BLOOM из petals"
      ],
      "metadata": {
        "id": "dOso3I-OazHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_NAME = \"bigscience/bloom-petals\"\n",
        "# tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "# model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME)\n",
        "# model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "zF02XH07PTNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример одного входного prompt c 2 CoT для модели генерации"
      ],
      "metadata": {
        "id": "fgd6jYlXa6yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompts = FormattedPrompts(\n",
        "    data_path=Path('CoTResearch/GSM8K_data/train_data.jsonl'),\n",
        "    n_exemplars=2, \n",
        "    random_seed=123\n",
        "    )\n",
        "example_prompts.sample_prompts()\n",
        "example_inputs = FormattedInputs(example_prompts)\n",
        "\n",
        "\n",
        "with open(Path('CoTResearch/GSM8K_data/test_data.jsonl')) as file:\n",
        "    example_prompt = example_inputs.sample_input(file.readline())\n",
        "print(example_prompt)"
      ],
      "metadata": {
        "id": "I34HjBP3Zqyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем объект форматированных prompts и делаю сэмплинг из тренировочного набора. Для проведения экспериментов устанавливаю некоторый seed для формирования экземпляров prompts и выбора тестовых вопросов из GSM8K"
      ],
      "metadata": {
        "id": "IMLDkq4RbFB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPTS_SET_SEED = 54321\n",
        "TEST_SPLIT_SEED = 77777\n",
        "prompts = FormattedPrompts(\n",
        "    data_path=Path('CoTResearch/GSM8K_data/train_data.jsonl'),\n",
        "    n_exemplars=5,\n",
        "    random_seed=PROMPTS_SET_SEED\n",
        "    )\n",
        "prompts.sample_prompts()\n",
        "inputs = FormattedInputs(prompts)\n",
        "\n",
        "\n",
        "# Build few-shot prompting subsample dataset\n",
        "N_DATA_POINTS = 100\n",
        "with open(Path('CoTResearch/GSM8K_data/test_data.jsonl')) as file:\n",
        "    lines = file.readlines()\n",
        "    np.random.seed(TEST_SPLIT_SEED)\n",
        "    data_points_indices = np.random.randint(0, len(lines), size=(N_DATA_POINTS, ))\n",
        "    for data_point_index in data_points_indices:\n",
        "        inputs.sample_input(lines[data_point_index])"
      ],
      "metadata": {
        "id": "96qpGyCqiVxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объект answers_list хранит отформатированные ответы модели и истинные ответы"
      ],
      "metadata": {
        "id": "iXQ0FXMgb5aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers_list = AnswersList()"
      ],
      "metadata": {
        "id": "k0l88-4xl9ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В Hugging Face Inference API использую аналогичную модель BLOOM 176B и свой токен для использования http API моделей "
      ],
      "metadata": {
        "id": "tgpfqBMzcDig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bloom\"\n",
        "API_URL = f\"https://api-inference.huggingface.co/models/bigscience/{MODEL_NAME}\"\n",
        "HEADERS = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "xU8E9ufuC07q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цикл получения генераций по всему тестовому набору данных при помощи сформированных prompts и жадной генерации. Использование HuggingFace Inference API"
      ],
      "metadata": {
        "id": "jn8JRl_PcaMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# greedy chain of though prompting\n",
        "# В цикле выполняется генерация и сохранение\n",
        "for i in tqdm(range(N_DATA_POINTS)):\n",
        "    time.sleep(1)\n",
        "    prompt = inputs.inputs[i]\n",
        "    gt_answer = inputs.ground_truths[i]\n",
        "    try:\n",
        "        output = query(\n",
        "            payload={\n",
        "                \"inputs\": prompt.strip(),\n",
        "                \"parameters\": {\n",
        "                    \"top_k\": None,\n",
        "                    \"top_p\": None,\n",
        "                    \"temperature\": 1,\n",
        "                    \"repetition_penalty\": None,\n",
        "                    \"max_new_tokens\": 249,\n",
        "                    \"max_time\": None,\n",
        "                    \"return_full_text\": False,\n",
        "                    \"num_return_sequences\": 1,\n",
        "                    \"do_sample\": False,\n",
        "                    \"stop\": [\"Q:\", \"\\n\\n\", \"A:\"]\n",
        "                },\n",
        "                \"options\": {\n",
        "                    \"use_cache\": True,\n",
        "                    \"wait_for_model\": True\n",
        "                }\n",
        "            }\n",
        "        )\n",
        "        output = output[0][\"generated_text\"].strip('Q:A:')\n",
        "        answers_list.add_answer(output, gt_answer)\n",
        "        print(output)\n",
        "    except Exception as e:\n",
        "        print(f'Exception occured on iteration {i}...{e}')\n",
        "        break\n",
        "\n",
        "answers_list.write_to_file(f'./drive/MyDrive/checkpoint{PROMPTS_SET_SEED}.jsonl')"
      ],
      "metadata": {
        "id": "sCMy29OmH0TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цикл получения генераций по всему тестовому набору данных при помощи сформированных prompts и жадной генерации. Использование petals distributed"
      ],
      "metadata": {
        "id": "xA_atRjCqEwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(N_DATA_POINTS)):\n",
        "    prompt = inputs.inputs[i]\n",
        "    gt_answer = inputs.ground_truths[i]\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
        "    outputs = model.generate(\n",
        "        tokenized_prompt,\n",
        "        max_new_tokens=128,\n",
        "        return_full_text=False,\n",
        "        stop=['\\n\\n', 'Q:'],\n",
        "        # num_return_sequences=1  # number of paths for ansembling\n",
        "    )\n",
        "    predicted_answer = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        # truncate_before_pattern=[r'\\n\\n', r'Q:']\n",
        "    )\n",
        "    answers_list.add_answer(predicted_answer, gt_answer)"
      ],
      "metadata": {
        "id": "q2l_7tb-PTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Анализ полученных результатов"
      ],
      "metadata": {
        "id": "glJcbEaAqQxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_result(filepath: str):\n",
        "    with open(filepath) as f:\n",
        "        return [json.loads(line) for line in f.readlines()]\n",
        "\n",
        "\n",
        "greedy_result1 = AnswersList(extract_result('./CoTResearch/experiments/greedy_results/1337.jsonl'))\n",
        "greedy_result2 = AnswersList(extract_result('./CoTResearch/experiments/greedy_results/42.jsonl'))\n",
        "greedy_result3 = AnswersList(extract_result('./CoTResearch/experiments/greedy_results/54321.jsonl'))\n",
        "greedy_result4 = AnswersList(extract_result('./CoTResearch/experiments/greedy_results/greedy_12345.jsonl'))\n",
        "greedy_result5 = AnswersList(extract_result('./CoTResearch/experiments/greedy_results/greedy_77777.jsonl'))\n",
        "standard_result = AnswersList(extract_result('./CoTResearch/experiments/standard_prompting_result/standard_prompting54321.jsonl'))\n",
        "greedy_results_list = [greedy_result1, greedy_result2, greedy_result3,\n",
        "                       greedy_result4, greedy_result5]"
      ],
      "metadata": {
        "id": "RK7Tm-iQqTtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_results_acc = list(map(lambda x: x.calculate_accuracy(), greedy_results_list))\n",
        "greedy_results_mean = np.mean(greedy_results_acc)\n",
        "greedy_results_std = np.std(greedy_results_acc)\n",
        "print('Среднее значение accuracy:', greedy_results_mean)\n",
        "print('Стандартное отклонение accuracy:', greedy_results_std)\n",
        "print(greedy_results_acc)\n",
        "standard_result_acc = standard_result.calculate_accuracy()\n",
        "print('Accuracy обыкновенного prompting:', standard_result_acc)"
      ],
      "metadata": {
        "id": "nu-RamScqYIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 9))\n",
        "plt.bar(range(len(greedy_results_acc)), greedy_results_acc)\n",
        "plt.bar(5, standard_result_acc, color='lightcoral')\n",
        "plt.xticks(range(len(greedy_results_acc) + 1), labels=['1337', '42', '54321', '12345', '77777', 'standard'])\n",
        "plt.ylim((0, 0.2))\n",
        "plt.title('Значение accuracy на тесте. Prompts с различными seed')\n",
        "plt.xlabel('random seed')\n",
        "plt.ylabel('Test accuracy');"
      ],
      "metadata": {
        "id": "cHAcei-AqYFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors_analysis_df = pd.DataFrame(\n",
        "    data=[[15, 7, 78, 100],\n",
        "          [15, 6, 79, 100],\n",
        "          [12, 9, 79, 100],\n",
        "          [11, 8, 81, 100],\n",
        "          [12, 9, 79, 100]],\n",
        "    index=pd.Series(['1337', '42', '54321', '12345', '77777'], name='random seed'),\n",
        "    columns=['Корректные', 'Маленькие допущения', 'Грубые ошибки', 'Всего']\n",
        ")\n",
        "errors_analysis_df['Количество CoT в одном prompt'] = 5\n",
        "errors_analysis_df.loc['standard prompting', :] = [6, 3, 91, 100, 0]\n",
        "errors_analysis_df"
      ],
      "metadata": {
        "id": "RURT5KsS0quT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Некоторые примеры небольших ошибок модели в арифметике и выводе"
      ],
      "metadata": {
        "id": "Pi_Siw1o5yX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{'predicted_answers': [' Argo has 200 - 40 - 80 - 30 = 30 toys. The answer is 30.\\n\\n'], 'predicted': ['30'], 'ground_truth': '50'}\n",
        "{'predicted_answers': [' 16 muffins = 16 x 5 = 80 tablespoons of flour. 16 muffins = 16 x 3 = 48 tablespoons of sugar. 16 muffins = 16 x 0.25 = 4 tablespoons of salt. 80 + 48 + 4 = 124 tablespoons of dry ingredients. The answer is 124.\\n\\n'], 'predicted': ['124'], 'ground_truth': '132'}\n",
        "{'predicted_answers': [' Jack had $100. Sophia gave him 1/5 of her $100. So Jack now has $100 + 1/5 * $100 = $105. The answer is 105.\\n\\n'], 'predicted': ['105'], 'ground_truth': '120'}\n",
        "{'predicted_answers': [' Adam had to contribute $30 - (6 + 2 x 6) = $14. The answer is 14.\\n\\n'], 'predicted': ['14'], 'ground_truth': '12'}\n",
        "{'predicted_answers': [' The total amount of money spent on functioning pieces of equipment is $400000 -.4 x $400000 = $300000. The answer is $300000.\\n\\n'], 'predicted': ['300000'], 'ground_truth': '240000'}\n",
        "{'predicted_answers': [' Joey has 214 + 26 = 240 points before his turn. Marcy has 225 + 10 = 235 points before her turn. Joey has 240 - 235 = 15 points more than Marcy. The answer is 15.\\n\\n'], 'predicted': ['15'], 'ground_truth': '5'}\n",
        "{'predicted_answers': [' The total number of posts in March is 1000 * 3 * 31 = 31,000.\\n'], 'predicted': ['31,000'], 'ground_truth': '93000'}"
      ],
      "metadata": {
        "id": "vEL59hU-vjhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZqqArFkjz13S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}