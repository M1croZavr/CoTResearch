{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPP3UUAaecmGvTsCE9eAm46",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1croZavr/CoTResearch/blob/master/CoT_greedy_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Установка и импортирование необходимых библиотек и git clone репозитория с необходимым кодом и данными"
      ],
      "metadata": {
        "id": "THdRTvEhacxv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PnfvS_aPQWX"
      },
      "outputs": [],
      "source": [
        "# %pip install -q petals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/M1croZavr/CoTResearch.git"
      ],
      "metadata": {
        "id": "OYIT9JN2ZfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "IjH5Z93JjAsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "# from transformers import BloomTokenizerFast, set_seed\n",
        "# from petals import DistributedBloomForCausalLM\n",
        "from CoTResearch.data_preprocessing import FormattedPrompts, FormattedInputs\n",
        "from CoTResearch.data_postprocessing import AnswersList"
      ],
      "metadata": {
        "id": "VMx-JO37PTJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running device: {DEVICE}')"
      ],
      "metadata": {
        "id": "c8KkQZwvZQLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка модели BLOOM из petals"
      ],
      "metadata": {
        "id": "dOso3I-OazHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL_NAME = \"bigscience/bloom-petals\"\n",
        "# tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "# model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME)\n",
        "# model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "zF02XH07PTNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример одного prompt c 2 CoT"
      ],
      "metadata": {
        "id": "fgd6jYlXa6yE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompts = FormattedPrompts(Path('CoTResearch/GSM8K_data/train_data.jsonl'), 2, 123)\n",
        "example_prompts.sample_prompts()\n",
        "example_inputs = FormattedInputs(example_prompts)\n",
        "\n",
        "\n",
        "with open(Path('CoTResearch/GSM8K_data/test_data.jsonl')) as file:\n",
        "    example_prompt = example_inputs.sample_input(file.readline())\n",
        "print(example_prompt)"
      ],
      "metadata": {
        "id": "I34HjBP3Zqyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем объект форматированных prompts и делаю сэмплинг из тренировочного набора. Для проведения экспериментов устанавливаю некоторый seed для формирования экземпляров prompts и выбора тестовых вопросов из GSM8K"
      ],
      "metadata": {
        "id": "IMLDkq4RbFB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 12345\n",
        "prompts = FormattedPrompts(\n",
        "    data_path=Path('CoTResearch/GSM8K_data/train_data.jsonl'),\n",
        "    n_exemplars=5,\n",
        "    random_seed=42\n",
        "    )\n",
        "prompts.sample_prompts()\n",
        "inputs = FormattedInputs(prompts)\n",
        "\n",
        "\n",
        "# Build few-shot prompting subsample dataset\n",
        "N_DATA_POINTS = 100\n",
        "with open(Path('CoTResearch/GSM8K_data/test_data.jsonl')) as file:\n",
        "    lines = file.readlines()\n",
        "    np.random.seed(SEED)\n",
        "    data_points_indices = np.random.randint(0, len(lines), size=(N_DATA_POINTS, ))\n",
        "    for data_point_index in data_points_indices:\n",
        "        inputs.sample_input(lines[data_point_index])"
      ],
      "metadata": {
        "id": "96qpGyCqiVxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объект answers_list хранит отформатированные ответы модели и истинные ответы"
      ],
      "metadata": {
        "id": "iXQ0FXMgb5aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answers_list = AnswersList()"
      ],
      "metadata": {
        "id": "k0l88-4xl9ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В Hugging Face Inference API использую аналогичную модель BLOOM 176B и свой токен для использования http API моделей "
      ],
      "metadata": {
        "id": "tgpfqBMzcDig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bloom\"\n",
        "API_URL = f\"https://api-inference.huggingface.co/models/bigscience/{MODEL_NAME}\"\n",
        "HEADERS = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "\n",
        "def query(payload):\n",
        "    response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
        "    return response.json()"
      ],
      "metadata": {
        "id": "xU8E9ufuC07q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Цикл получения генераций по всему тестовому набору данных при помощи сформированных prompts и жадной генерации"
      ],
      "metadata": {
        "id": "jn8JRl_PcaMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# greedy chain of though prompting\n",
        "for i in tqdm(range(5, N_DATA_POINTS)):\n",
        "    time.sleep(5)\n",
        "    prompt = inputs.inputs[i]\n",
        "    gt_answer = inputs.ground_truths[i]\n",
        "    output = query(\n",
        "        payload={\n",
        "            \"inputs\": prompt.strip(),\n",
        "            \"parameters\": {\n",
        "                \"top_k\": None,\n",
        "                \"top_p\": None,\n",
        "                \"temperature\": 1,\n",
        "                \"repetition_penalty\": None,\n",
        "                \"max_new_tokens\": 250,\n",
        "                \"max_time\": None,\n",
        "                \"return_full_text\": False,\n",
        "                \"num_return_sequences\": 1,\n",
        "                \"do_sample\": False,\n",
        "                \"stop\": [\"Q:\", \"\\n\\n\"]\n",
        "            },\n",
        "            \"options\": {\n",
        "                \"use_cache\": True,\n",
        "                \"wait_for_model\": True\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    print(output)\n",
        "    answers_list.add_answer(output[0][\"generated_text\"], gt_answer)\n",
        "answers_list.write_to_file('./greedy_42_seed.jsonl')"
      ],
      "metadata": {
        "id": "sCMy29OmH0TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.inputs[0]"
      ],
      "metadata": {
        "id": "7omtP-NzOGnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in zip(inputs.ground_truths, answers_list):\n",
        "    print(i)\n",
        "    print(j, end='\\n\\n')"
      ],
      "metadata": {
        "id": "diYEXH-EJ7FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(answers_list)"
      ],
      "metadata": {
        "id": "NtsBOfduURDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('./greedy_12345_seed.jsonl') as file:\n",
        "    for line in file.readlines():\n",
        "        print(line)"
      ],
      "metadata": {
        "id": "OzhaawHTQHwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(N_DATA_POINTS)):\n",
        "    prompt = inputs.inputs[i]\n",
        "    gt_answer = inputs.ground_truths[i]\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
        "    outputs = model.generate(\n",
        "        tokenized_prompt,\n",
        "        max_new_tokens=128,\n",
        "        return_full_text=False,\n",
        "        stop=['\\n\\n', 'Q:'],\n",
        "        # num_return_sequences=1  # number of paths for ansembling\n",
        "    )\n",
        "    predicted_answer = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        # truncate_before_pattern=[r'\\n\\n', r'Q:']\n",
        "    )\n",
        "    answers_list.add_answer(predicted_answer, gt_answer)\n",
        "\n",
        "# payload = {\n",
        "# \"inputs\": promt,\n",
        "# \"parameters\": {\n",
        "# \"do_sample\": True,\n",
        "# \"top_p\": X,\n",
        "# \"max_new_tokens\": 150,\n",
        "# \"temperature\": X,\n",
        "# \"stop\": ['.', 'The next day']\n",
        "# }"
      ],
      "metadata": {
        "id": "q2l_7tb-PTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BInC2G41ua_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}