{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPldtlyZhv5LO/IgSV281ho",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M1croZavr/CoTResearch/blob/master/CoT_greedy_research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PnfvS_aPQWX"
      },
      "outputs": [],
      "source": [
        "%pip install -q petals"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/M1croZavr/CoTResearch.git"
      ],
      "metadata": {
        "id": "OYIT9JN2ZfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "id": "IjH5Z93JjAsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from transformers import BloomTokenizerFast, set_seed\n",
        "from petals import DistributedBloomForCausalLM\n",
        "from CoTResearch.data_preprocessing import FormattedPrompts, FormattedInputs\n",
        "from CoTResearch.data_postprocessing import AnswersList"
      ],
      "metadata": {
        "id": "VMx-JO37PTJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Running device: {DEVICE}')"
      ],
      "metadata": {
        "id": "c8KkQZwvZQLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"bigscience/bloom-petals\"\n",
        "tokenizer = BloomTokenizerFast.from_pretrained(MODEL_NAME)\n",
        "model = DistributedBloomForCausalLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)"
      ],
      "metadata": {
        "id": "zF02XH07PTNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_prompts = FormattedPrompts(Path('CoTResearch/GSM8K_data/train_data.jsonl'), 3, 123)\n",
        "example_inputs = FormattedInputs(example_prompts)\n",
        "with open(Path('CoTResearch/GSM8K_data/test_data.jsonl')) as file:\n",
        "    example_prompt = example_inputs.sample_input(file.readline())\n",
        "print(example_prompt)"
      ],
      "metadata": {
        "id": "I34HjBP3Zqyz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = FormattedPrompts(\n",
        "    data_path=Path('CoTResearch/GSM8K_data/train_data.jsonl'),\n",
        "    n_exemplars=8,\n",
        "    random_seed=123\n",
        "    )\n",
        "prompts.sample_prompts()\n",
        "inputs = FormattedInputs(prompts)\n",
        "\n",
        "# Build few-shot prompting mini-sample dataset\n",
        "N_DATA_POINTS = 50\n",
        "with open(Path('CoTResearch/GSM8K_data/test_data.jsonl')) as file:\n",
        "    lines = file.readlines()\n",
        "    data_points_indices = np.random.randint(0, len(lines), size=(N_DATA_POINTS, ))\n",
        "    for data_point_index in data_points_indices:\n",
        "        inputs.sample_input(lines[data_point_index])"
      ],
      "metadata": {
        "id": "96qpGyCqiVxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answers_list = AnswersList()"
      ],
      "metadata": {
        "id": "k0l88-4xl9ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(inputs.inputs), len(inputs.ground_truths)"
      ],
      "metadata": {
        "id": "zL424fWYk-AX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(N_DATA_POINTS)):\n",
        "    prompt = inputs.inputs[i]\n",
        "    gt_answer = inputs.ground_truths[i]\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(DEVICE)\n",
        "    outputs = model.generate(\n",
        "        tokenized_prompt,\n",
        "        max_new_tokens=128,\n",
        "        return_full_text=False,\n",
        "        stop=['\\n\\n', 'Q:'],\n",
        "        # num_return_sequences=1  # number of paths for ansembling\n",
        "    )\n",
        "    predicted_answer = tokenizer.decode(\n",
        "        outputs[0],\n",
        "        # truncate_before_pattern=[r'\\n\\n', r'Q:']\n",
        "    )\n",
        "    answers_list.add_answer(predicted_answer, gt_answer)\n",
        "\n",
        "# payload = {\n",
        "# \"inputs\": promt,\n",
        "# \"parameters\": {\n",
        "# \"do_sample\": True,\n",
        "# \"top_p\": X,\n",
        "# \"max_new_tokens\": 150,\n",
        "# \"temperature\": X,\n",
        "# \"stop\": ['.', 'The next day']\n",
        "# }"
      ],
      "metadata": {
        "id": "q2l_7tb-PTP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_answer"
      ],
      "metadata": {
        "id": "Q-xxuf31PTSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "id": "H57MpTaTeMti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.inputs[0]"
      ],
      "metadata": {
        "id": "qR3lYvF6toaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.ground_truths[0]"
      ],
      "metadata": {
        "id": "2yOzq4WDuB9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts.prompts"
      ],
      "metadata": {
        "id": "xpPkce1auFcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BInC2G41ua_E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}